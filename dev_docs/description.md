0. Предпосылки и допущения

Чтобы не зависнуть на уточнениях, зафиксируем базовые решения (их можно менять):
	1.	Язык и фреймворк бэка:
	•	Python 3.11+
	•	FastAPI для HTTP API
	2.	LLM и эмбеддинги:
	•	LLM: OpenAI gpt-4.1-mini или аналог через openai/openai SDK (Completions / Chat Completions, как дадут).
	•	Эмбеддинги: OpenAI text-embedding-3-small (маленькая, дешевая, нормальная для RAG).
	3.	Векторное хранилище:
	•	Базовый вариант: Chroma (локально, без внешней инфраструктуры).
	•	Архитектурно — можно подменить на Qdrant/FAISS через абстракцию VectorStore.
	4.	Формат корпуса:
	•	3 файла в data/corpus/:
	•	lotr_fellowship.txt
	•	lotr_two_towers.txt
	•	lotr_return_of_king.txt
	•	Внутри — текст с разметкой глав по простому паттерну (например, ГЛАВА 1. Название).
	5.	Аутентификация:
	•	Для тестового задания — нет пользовательской аутентификации.
	•	Админ-эндпоинты защищаем простым ADMIN_TOKEN в заголовке.

Если что-то из этого не подходит — потом просто переопределите раздел.

⸻

1. Цель системы

Создать Python-сервис с HTTP API, который:
	1.	Индексирует корпус текстов «Властелин колец» (3 книги, русская версия) в векторное хранилище.
	2.	Отвечает на вопросы пользователя строго по корпусу (RAG).
	3.	При недостаточном/нерелевантном контексте честно отказывается отвечать.
	4.	Возвращает:
	•	краткий ответ (≤1500 символов);
	•	источники (книга, глава, позиция);
	•	короткую цитату (1–2 предложения) и/или расширенный контекст для фронта.

⸻

2. Высокоуровневая архитектура

2.1. Компоненты
	1.	REST API (FastAPI)
	•	Эндпоинт для вопросов пользователя.
	•	Админ-эндпоинт/CLI для переиндексации.
	•	Health-check.
	2.	RAG-сервис (доменный слой)
	•	Нормализация вопроса.
	•	Поиск релевантных фрагментов в векторном индексе.
	•	Формирование промпта.
	•	Вызов LLM.
	•	Guardrails (до и/или после LLM).
	3.	Модуль индексации корпуса
	•	Загрузка файлов книг.
	•	Парсинг книги → главы → текст.
	•	Очистка текста, нормализация.
	•	Чанкинг на фрагменты с перекрытием.
	•	Генерация embeddings.
	•	Запись в векторное хранилище.
	4.	Векторное хранилище
	•	Обёртка над Chroma | Qdrant | FAISS.
	•	API: upsert_documents, search(query_embedding, top_k).
	5.	Инфраструктурная обвязка
	•	Конфиг через env / .env.
	•	Логирование.
	•	Обработка ошибок.

⸻

3. Модуль индексации корпуса

3.1. Формат данных

Документ (фрагмент) в индексе:

{
  "id": "lotr_fellowship_ch1_0001",
  "text": "Фрагмент текста...",
  "metadata": {
    "book": "The Fellowship of the Ring",
    "book_id": "fellowship",
    "chapter_title": "Глава 1. Название главы",
    "chapter_index": 1,
    "chunk_index": 1,
    "position": 0,                 // смещение в символах или токенах
    "source_file": "lotr_fellowship.txt"
  },
  "embedding": [ ... ]
}

Обязательные метаданные (по ТЗ):
	•	book
	•	chapter (здесь: chapter_title и/или chapter_index)
	•	position (например, смещение в символах от начала главы)

3.2. Этапы индексации
	1.	Загрузка файлов
	•	Путь до директории корпуса задаётся через CORPUS_DIR.
	•	Поддерживаем .txt (минимум).
	•	Маппинг файл → книга:
	•	lotr_fellowship.txt → "The Fellowship of the Ring"
	•	и т. д. (жёстко прописано в конфиге).
	2.	Парсинг структуры книги
	•	Простая эвристика: глава начинается с строки, удовлетворяющей паттерну, например:
	•	^ГЛАВА\s+\d+ или
	•	^ЧАСТЬ\s+\d+ / ^КНИГА\s+\d+.
	•	Для тестового задания достаточно поддержать один формат; но в архитектуре выделяем интерфейс BookParser.
	3.	Очистка и нормализация текста
	•	Удалить лишние пробелы, дублирующиеся переносы строк.
	•	Убрать служебные маркеры (если есть).
	•	Нормализовать кавычки, длинные тире, и т. п. (опционально).
	•	Сохранить структуру: книга → глава → сплошной текст.
	4.	Чанкинг
	•	Цель: разбить текст глав на фрагменты подходящего размера для LLM-контекста.
	•	Параметры (конфиг):
	•	CHUNK_SIZE_CHARS (например, 800–1200 символов).
	•	CHUNK_OVERLAP_CHARS (например, 200–300 символов).
	•	Алгоритм:
	•	идём по тексту главы с шагом CHUNK_SIZE - CHUNK_OVERLAP.
	•	каждый фрагмент получает chunk_index и position (смещение).
	5.	Генерация эмбеддингов
	•	Модуль EmbeddingsClient, который:
	•	принимает список текстов (List[str]);
	•	возвращает список векторов (List[List[float]]);
	•	реализован через OpenAI Embeddings API.
	•	Параметры:
	•	EMBEDDING_MODEL_NAME (env).
	•	Батчинг: по 64/128 фрагментов за запрос для эффективности.
	6.	Запись в векторное хранилище
	•	Абстракция VectorStore:
	•	clear() — полностью очищает индекс.
	•	upsert_documents(documents: List[DocumentChunk]).
	•	Для Chroma:
	•	создаём коллекцию lotr_corpus.
	•	ключевой ID — id.

3.3. Переиндексация одной командой

Вариант 1. CLI (рекомендуется)

Скрипт scripts/reindex_corpus.py:

python -m app.scripts.reindex_corpus --rebuild

Поведение:
	1.	Поднимает соединение к векторному хранилищу.
	2.	Вызывает VectorStore.clear().
	3.	Идёт по всем файлам в CORPUS_DIR.
	4.	Индексирует.
	5.	Логирует прогресс.

Вариант 2. HTTP-эндпоинт (для удобства)

POST /admin/reindex
	•	Требует заголовок X-Admin-Token: <ADMIN_TOKEN>.
	•	Асинхронно запускает задачу по переиндексации (или синхронно для простоты, если объём небольшой).
	•	Возвращает JSON со статусом.

⸻

4. RAG-пайплайн

4.1. Общий сценарий
	1.	Получаем вопрос пользователя.
	2.	Нормализуем вопрос (опционально).
	3.	Генерируем эмбеддинг вопроса.
	4.	Ищем top-K релевантных фрагментов в векторном хранилище.
	5.	Применяем эвристику релевантности / порог по similaritу.
	6.	Если релевантность низкая → отказ без вызова LLM.
	7.	Иначе формируем промпт = инструкции + контекст + вопрос.
	8.	Отправляем в LLM.
	9.	Применяем post-guardrails (проверка ответа на «галлюцинации»).
	10.	Возвращаем клиенту структурированный ответ.

4.2. Нормализация вопроса (опционально)

Функция normalize_question(text: str) -> str:
	•	Трим пробелов.
	•	Удаление очевидного мусора (\n\n, лишние спецсимволы).
	•	(Опционально) приведение к нижнему регистру — но лучше не трогать, чтобы не портить имена собственные.
	•	Можно добавить простой фильтр языков (если вопрос не на русском → отказ).

4.3. Поиск релевантных фрагментов
	1.	Считаем эмбеддинг вопроса через EmbeddingsClient.
	2.	Вызываем VectorStore.search(embedding, top_k=K):
	•	K по конфигу (например, 5–8).
	•	Возвращаем список: [(text, metadata, score), ...].
	3.	Порог релевантности:
	•	Параметр RELEVANCE_THRESHOLD (например, 0.75–0.8 для cosine similarity).
	•	Если max_score < threshold → считаем, что корпус не содержит достаточной инфы.
	4.	(Опционально) перекомпозиция контекста:
	•	Сортировка по score.
	•	Слияние кусочков из одной и той же главы, если они идут подряд.

4.4. Guardrails до LLM

Условия отказа до LLM:
	•	max_score < threshold.
	•	или количество фрагментов с score >= threshold < MIN_GOOD_CHUNKS.

В этом случае возвращаем стандартную формулировку:

«В загруженных текстах недостаточно информации для точного ответа на этот вопрос.»

4.5. Формирование промпта

Используем системное сообщение + один запрос пользователя (формат для Chat Completions).

System prompt (пример, рус.):

Ты — помощник по книгам «Властелин колец».
Ты отвечаешь строго на основе предоставленных фрагментов текста.
Если в фрагментах недостаточно данных для точного ответа, ты обязан честно написать:
«В загруженных текстах недостаточно информации для точного ответа».
Нельзя придумывать факты, которые не подтверждаются фрагментами.
Отвечай кратко и по делу, не более 1500 символов для основного текста ответа.
Кроме основного текста ответа, ты должен указать книгу, главу/часть и привести короткую цитату (1–2 предложения), которые подтверждают ответ.
Если контекст не относится к вопросу — также честно отказывайся отвечать.

User message (пример):

Вопрос пользователя: "<оригинальный вопрос>"

Ниже приведены фрагменты из книги. Используй только их.

[Фрагмент 1]
Книга: <book>, Глава: <chapter>, Позиция: <position>
Текст:
"<chunk_text_1>"

[Фрагмент 2]
...

Требуемый формат ответа (JSON):
{
  "answer_short": "Краткий ответ на вопрос, не более 1500 символов.",
  "answer_full": "Более развернутый ответ (можно частично повторять краткий).",
  "sources": [
    {
      "book": "Название книги",
      "chapter": "Название или номер главы",
      "position": <число>,
      "quote": "Короткая цитата 1-2 предложения из текста фрагмента."
    }
  ],
  "can_answer": true|false
}
Если информации недостаточно, верни:
{
  "answer_short": "В загруженных текстах недостаточно информации для точного ответа.",
  "answer_full": "В загруженных текстах недостаточно информации для точного ответа.",
  "sources": [],
  "can_answer": false
}

Таким образом мы жёстко задаём формат и ответственность за отказ.

4.6. Вызов LLM
	•	Клиент LLMClient (обёртка над OpenAI).
	•	Параметры:
	•	LLM_MODEL_NAME
	•	LLM_TEMPERATURE (низкая, типа 0–0.2, чтобы уменьшить галлюцинации).
	•	Таймауты, ретраи.

4.7. Post-Guardrails

Так как мы уже:
	•	ограничили контекст фрагментами;
	•	описали строгий JSON-формат;
	•	попросили флаг can_answer;

Дополнительно можно:
	1.	Проверка структуры JSON:
	•	Парсим строку → JSON;
	•	Проверяем наличие полей.
	2.	Правило безопасности:
	•	Если can_answer == false → считаем отказ корректным.
	•	Если can_answer == true, но при этом:
	•	sources пустой;
	•	или answer_short слишком длинный;
	•	или строка не содержит ничего из ключевых слов из контекста —
тогда лучше принудительно заменить ответ на отказ.
	3.	(Опционально) второй LLM-запрос на валидацию:
	•	Для тестового задания можно опустить.

⸻

5. Guardrails (подробно)

5.1. На уровне конфигурации LLM
	•	Температура 0–0.2;
	•	Ясные инструкции «не придумывать факты»;
	•	Формат ответа строго в JSON.

5.2. На уровне retrieval
	•	Порог релевантности.
	•	Отсечение запросов, которые явно не про LOTR (эвристика/классификатор по LLM: «относится ли вопрос к “Властелину Колец”? (да/нет)»).

5.3. На уровне бизнес-логики
	•	Если sources пустые или can_answer=false → отображаем отказ.
	•	Если распарсить ответ как JSON не удалось → fallback:
	•	вернуть отказ с текстом по умолчанию.

⸻

6. Цитирование и формат ответа

Требования ТЗ:
	1.	Краткая формулировка ответа.
	2.	Указание книги и главы/части.
	3.	Желательно 1–2 предложения цитаты.
	4.	Ограничение ответа до 1500 символов.

6.1. Формат ответа бэка

Ответ API (для фронта) предлагаю сделать таким:

{
  "answer_short": "Краткий ответ...",
  "answer_full": "Более развернутый ответ...",
  "citations": [
    {
      "book": "The Fellowship of the Ring",
      "book_id": "fellowship",
      "chapter_title": "Глава 1. Название",
      "chapter_index": 1,
      "position": 1234,
      "quote": "Короткая цитата 1–2 предложения.",
      "chunk_id": "lotr_fellowship_ch1_0001"
    }
  ],
  "context_chunks": [
    {
      "chunk_id": "lotr_fellowship_ch1_0001",
      "text": "Полный текст чанка...",
      "metadata": { ... }
    }
  ],
  "can_answer": true
}

Фронт сможет:
	•	показывать answer_short сразу;
	•	по кнопке — расширенный answer_full;
	•	по другой кнопке — quote или context_chunks.

⸻

7. API-спецификация

7.1. POST /api/v1/ask

Назначение: основной эндпоинт для вопросов пользователя.

Request:

{
  "question": "Текст вопроса",
  "mode": "default",         // опционально: "short_only", "debug"
  "max_context_chunks": 5    // опционально, по умолчанию 5
}

Response 200:

{
  "answer_short": "Краткий ответ...",
  "answer_full": "Полный/расширенный ответ...",
  "can_answer": true,
  "citations": [
    {
      "book": "The Fellowship of the Ring",
      "book_id": "fellowship",
      "chapter_title": "Глава 1. Название",
      "chapter_index": 1,
      "position": 1234,
      "quote": "Короткая цитата..."
    }
  ],
  "context_chunks": [
    {
      "chunk_id": "lotr_fellowship_ch1_0001",
      "text": "Полный текст чанка...",
      "metadata": { ... }
    }
  ],
  "raw_scores": [
    {
      "chunk_id": "lotr_fellowship_ch1_0001",
      "score": 0.89
    }
  ]
}

Response при невозможности ответа (guardrails):

{
  "answer_short": "В загруженных текстах недостаточно информации для точного ответа.",
  "answer_full": "В загруженных текстах недостаточно информации для точного ответа.",
  "can_answer": false,
  "citations": [],
  "context_chunks": []
}

Ошибки:
	•	400 Bad Request — пустой вопрос.
	•	500 Internal Server Error — проблемы с LLM/векторным индексом.

⸻

7.2. POST /admin/reindex

Назначение: полная переиндексация корпуса.

Headers:
	•	X-Admin-Token: <ADMIN_TOKEN>

Request:

{
  "mode": "full"        // на будущее: "full", "incremental"
}

Response 202/200:

{
  "status": "started",
  "details": "Reindexing started"
}

или по завершению синхронно:

{
  "status": "completed",
  "indexed_chunks": 1234
}


⸻

7.3. GET /health

Назначение: проверка живости сервиса.

Response 200:

{
  "status": "ok",
  "vector_store": "ok",
  "llm": "ok"
}


⸻

8. Конфигурация и окружение

Все чувствительные и параметризуемые настройки — через env.

Пример .env:

OPENAI_API_KEY=...
LLM_MODEL_NAME=gpt-4.1-mini
EMBEDDING_MODEL_NAME=text-embedding-3-small

VECTOR_STORE_BACKEND=chroma
VECTOR_STORE_PATH=./data/vector_store

CORPUS_DIR=./data/corpus

RELEVANCE_THRESHOLD=0.78
MIN_GOOD_CHUNKS=2
MAX_CONTEXT_CHUNKS=5

ADMIN_TOKEN=supersecretadmin

APP_HOST=0.0.0.0
APP_PORT=8000


⸻

9. Нефункциональные требования
	1.	Markdown-форматирование:
	•	answer_short и answer_full допускают markdown (списки, выделение, цитаты >).
	•	Цитаты оформлять как:

> "Текст цитаты..."
> (*Книга*, глава N)


	2.	Обработка неожиданных вводов:
	•	Пустой вопрос → 400 с сообщением.
	•	Неверный JSON → 400.
	•	Прочие ошибки → 500 с безопасным сообщением, логи — в stderr.
	3.	Логирование:
	•	Логировать:
	•	запросы (id, время);
	•	результаты поиска (id чанков и score);
	•	ошибки LLM/VectorStore.
	4.	Тестирование:
	•	Юнит-тесты для:
	•	чанкинга;
	•	индексации;
	•	поиска по эмбеддингам (с моками);
	•	guardrails.
	•	Минимальный набор интеграционных тестов на RAG-пайплайн.

⸻

10. Типовая схема пайплайна (по шагам)

10.1. Сценарий «Переиндексация»
	1.	Админ вызывает POST /admin/reindex или CLI-скрипт.
	2.	Сервис:
	•	очищает коллекцию в векторном стораже;
	•	идёт по файлам в CORPUS_DIR;
	•	парсит книгу на главы;
	•	чанкает текст;
	•	считает эмбеддинги;
	•	записывает все чанки в индекс.
	3.	Логи фиксируют кол-во книг, глав, чанков, время.

10.2. Сценарий «Пользователь задаёт вопрос»
	1.	Фронт вызывает POST /api/v1/ask с вопросом.
	2.	Бэк:
	•	нормализует вопрос;
	•	генерирует эмбеддинг вопроса;
	•	ищет top-K чанков;
	•	проверяет порог релевантности;
	•	при низкой релевантности → формирует отказ и отдаёт;
	•	при достаточной релевантности:
	•	формирует промпт (JSON-инструкции);
	•	вызывает LLM;
	•	парсит JSON, проверяет can_answer и sources;
	•	при проблеме — fallback к отказу;
	•	отдаёт фронту JSON с ответом и цитатами.

⸻

11. Что разработчикам нужно реализовать по модулям
	1.	app/config.py
	•	Чтение env и валидация параметров.
	2.	app/vector_store/base.py
	•	Интерфейс VectorStore.
	3.	app/vector_store/chroma_store.py
	•	Реализация VectorStore для Chroma.
	4.	app/embeddings/client.py
	•	Клиент для OpenAI Embeddings.
	5.	app/llm/client.py
	•	Клиент для OpenAI LLM (chat/completions).
	6.	app/indexing/parser.py
	•	Парсер книг и глав.
	7.	app/indexing/chunker.py
	•	Чанкинг текста.
	8.	app/indexing/pipeline.py
	•	high-level функция reindex_corpus().
	9.	app/rag/pipeline.py
	•	High-level RAG-пайплайн: answer_question(question: str).
	10.	app/api/routes.py
	•	FastAPI роуты:
	•	/api/v1/ask
	•	/admin/reindex
	•	/health
	11.	scripts/reindex_corpus.py
	•	CLI для переиндексации.

⸻